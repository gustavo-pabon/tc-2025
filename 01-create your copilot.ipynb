{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78c15da-fdc1-470d-896e-7fcf06ac7d0d",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "    <img \n",
    "        src=\"./img/Microsoft-Logo.png\" \n",
    "        width=\"400\"/>\n",
    "</h1>\n",
    "<h1 align=\"center\">\n",
    "    <b>Practical Guide</b>\n",
    "</h1>\n",
    "<h4 align=\"center\">\n",
    "    for the creation of an AI Solution using an accelerator from the <a href=\"https://www.ds-toolkit.com/\">Data Science Toolkit</a>\n",
    "</h4>\n",
    "\n",
    "# What to expect\n",
    "\n",
    "* **Challenge 1:** *Create your own AI solution*\n",
    "* **Challenge 2:** *Getting answers from our new Copilot*\n",
    "* **Challenge 3:** *Evaluate the quality of the AI solution*\n",
    "\n",
    "# Challenge 1: *Create your own AI solution*\n",
    "\n",
    "Here we are going to create a RAG based Copilot to answer questions about 6 sustainability reports from Microsoft, Apple, Amazon, Google, Meta and Netflix from 2021 or 2022. The documents are part of the  [Mini Esg Bench Dataset](https://llamahub.ai/l/llama_datasets/Mini%20ESG%20Bench%20Dataset?from=llama_datasets).\n",
    "\n",
    "## Challenge 1 - Step 1:  *Let's import the libraries to be used in this notebook*.\n",
    "\n",
    "> This step will take around **30 to 90 seconds** to complete. It is going to be done in a quiet mode, so only errors will be displayed (if they occur). If you want to see what is going to be installed check the [requirements.txt](./requirements.txt) file.\n",
    "\n",
    "In summary two main tools will be installed and used in this notebook:\n",
    "\n",
    "* **Llama Index**. Which will be used to download the dataset and to create the Semantic Index.\n",
    "  > It is also possible to use **Azure AI Search** to create the semantic index. However, since it is going to be a small index, to simplify its creation in-memory we are going to use Llama Index.\n",
    "* **RAGAS**. Ragas will be used to calculate the metrics for the Copilot that we are going to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22fcb7-9752-4a1b-b168-c2e46758e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# To create the RAG based copilot\n",
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ContextPrecision,\n",
    "    ContextRecall\n",
    ")\n",
    "\n",
    "# To calculate the Generative AI quality metrics\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from ragas.embeddings import LlamaIndexEmbeddingsWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "from ragas.evaluation import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# To create a simple PDF visualization tool\n",
    "import pymupdf\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# For formatting purposes\n",
    "import textwrap\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4bf4c3-9881-4fd0-a1e7-a81c0a5531f5",
   "metadata": {},
   "source": [
    "## Challenge 1 - Step 2: *Let's load the documents and test questions*\n",
    "\n",
    "> This step takes around **1 minute** to complete.\n",
    "\n",
    "The following code loads and prepares data from different sources (a JSON file and a directory of files) for further use:\n",
    "1. The first line creates an instance of LabelledRagDataset by loading data from a JSON file `rag_dataset.json`. The `from_json` method is a class method that reads the JSON file, parses its content, and initializes the `LabelledRagDataset` object with the parsed data. This method ensures that the data is correctly formatted and validated before being used.\n",
    "2. The second line initializes an instance of `SimpleDirectoryReader` with the directory path `source_files`. The `SimpleDirectoryReader` class is designed to read files from a specified directory. The `load_data` method is then called on this instance to load the documents from the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086796a-89cd-47dc-a54c-aca6d095e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the code below\n",
    "\n",
    "rag_dataset = LabelledRagDataset.from_json(\"./data/rag_dataset.json\")\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data/source_files/\").load_data(\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09182d0c-5b3c-4d9b-a7c6-ac6a04790b32",
   "metadata": {},
   "source": [
    "### Tool to visualize the reports just downloaded\n",
    "\n",
    "The following cell creates a tool to visualize the PDF files we just downloaded. If you want to take a look at the downloaded reports manually, just navigate to the `data/source_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b73ae8-1733-4654-b9fe-f64a366d4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path = \"./data/source_files\"\n",
    "\n",
    "# List of pdf files just downloaded\n",
    "pdf_files = [os.path.join(data_source_path, file_name) for file_name in os.listdir(data_source_path)]\n",
    "\n",
    "# Function to render a specific page of a PDF\n",
    "def render_pdf_page(pdf_path, page_number=0):\n",
    "    # Open the PDF file\n",
    "    pdf_document = pymupdf.open(pdf_path)\n",
    "    \n",
    "    # Ensure the page number is valid\n",
    "    if page_number < 0 or page_number >= len(pdf_document):\n",
    "        raise ValueError(\"Invalid page number.\")\n",
    "    \n",
    "    # Get the page and render it as an image\n",
    "    page = pdf_document[page_number]\n",
    "    pix = page.get_pixmap()\n",
    "    pdf_document.close()\n",
    "    \n",
    "    # Display the image using Matplotlib\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(pix.pil_image())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Function to update the displayed page\n",
    "def update_page(step):\n",
    "    global current_page\n",
    "    pdf_document = pymupdf.open(dropdown.value)\n",
    "    total_pages = len(pdf_document)\n",
    "    pdf_document.close()\n",
    "    \n",
    "    # Update the current page index\n",
    "    current_page += step\n",
    "    if current_page < 0:\n",
    "        current_page = 0\n",
    "    elif current_page >= total_pages:\n",
    "        current_page = total_pages - 1\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        render_pdf_page(dropdown.value, current_page)\n",
    "\n",
    "# Function to reset the viewer when a new PDF is selected\n",
    "def reset_viewer(change):\n",
    "    global current_page\n",
    "    current_page = 0  # Reset to the first page\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        render_pdf_page(dropdown.value, current_page)\n",
    "\n",
    "# Create widgets\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=pdf_files,\n",
    "    description=\"Select a PDF:\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "prev_button = widgets.Button(description=\"Previous Page\")\n",
    "next_button = widgets.Button(description=\"Next Page\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Attach event listeners\n",
    "prev_button.on_click(lambda _: update_page(-1))\n",
    "next_button.on_click(lambda _: update_page(1))\n",
    "dropdown.observe(reset_viewer, names=\"value\")\n",
    "\n",
    "# Initial display\n",
    "reset_viewer(None)\n",
    "\n",
    "# Display widgets and output\n",
    "display(widgets.VBox([dropdown, widgets.HBox([prev_button, next_button]), output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27593f21-4b06-4f54-bf8e-b7c7df1fa46c",
   "metadata": {},
   "source": [
    "## Challenge 1 - Step 3: *Let's create the semantic index*\n",
    "\n",
    "> This process can take up to **15 seconds** to complete\n",
    "\n",
    "A semantic index is a method used to organize and retrieve information based on the meaning and context of the content rather than just keywords. Unlike traditional keyword-based search systems, a semantic index understands the relationships between concepts and terms within the data, allowing for more accurate and relevant search results. In this context (i.e., ML and NLP), a semantic index leverages embeddings, which are numerical representations of words, phrases, or documents in a high-dimensional space. These embeddings capture the semantic meaning of the content, enabling the system to identify similar or related items even if they do not share exact keywords. The following code creates a semantic index using Azure OpenAI embeddings. The embedding model is used to generate embeddings for the documents, which are then indexed using `VectorStoreIndex` from `llamaIndex`. This process allows for efficient and meaningful retrieval of information based on the semantic content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7734c8e-3326-4a82-b1a6-7e5db3db1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the embedding model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model='text-embedding-3-small', # Update with the embeddings deployment name\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    api_version=os.environ['OPENAI_API_VERSION'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']\n",
    ")\n",
    "\n",
    "# Pass the embedding model to llamaIndex\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Create the actual index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf8b61-3d12-4e5d-8b9e-e852897b9585",
   "metadata": {},
   "source": [
    "## Challenge 1 - Step 4: *Let's create the copilot*\n",
    "\n",
    "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation, translation, summarization, and more. These models are characterized by their vast number of parameters, which enable them to understand and generate human-like text with high accuracy and fluency. LLMs are trained on extensive datasets using self-supervised learning, allowing them to acquire a deep understanding of language, including syntax, semantics, and contextual relationships.\n",
    "\n",
    "Here we are creating an instance of a large language model using Azure OpenAI. The model, identified as `gpt-4o`, is configured with specific parameters such as `engine`, `model`, `temperature`, and API credentials. This setup allows the model to generate text based on the input it receives, making it a powerful tool for various natural language processing tasks.\n",
    "\n",
    "The following code creates and configures the large language model using Azure OpenAI, and integrates it with the previously created semantic index to enable efficient and meaningful query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8b041-6ef4-40be-a2be-70828a2865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Large Language Model\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"gpt-4o\", # Update with the language model deployment name \n",
    "    model=\"gpt-4o\", # Update with the language model name\n",
    "    temperature=0.0,\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    api_version=os.environ['OPENAI_API_VERSION'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "\n",
    "query_engine = index.as_query_engine() # this is the copilot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91738a2c-4eae-4734-9748-5dff74d8b87d",
   "metadata": {},
   "source": [
    "### Let's play with the copilot just created\n",
    "\n",
    "In the test dataset, we have not only downloaded the source data (PDFs), but also 50 examples of questions/answer pairs to use with our Copilot. The following are some examples of the questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aafd0-1680-4871-801e-33be32930ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "\n",
    "# Load some questions as example\n",
    "samples = rag_dataset.to_pandas().loc[:num_samples-1, 'query'].values\n",
    "\n",
    "display(Markdown(\"\\n\".join([ \"* \" + sample for sample in samples])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5218b-6e99-49fb-b17e-67cb54a6b707",
   "metadata": {},
   "source": [
    "The following is a simple tool that allows to ask questions to the new copilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416fd98-37ef-41ba-88c2-c665c210b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a response\n",
    "def get_response(user_input):\n",
    "    return query_engine.query(user_input).response\n",
    "\n",
    "# Interactive chatbot function\n",
    "def chatbot():\n",
    "    output = widgets.Output()\n",
    "    scrollable_chat = widgets.Box(\n",
    "        [output],\n",
    "        layout=widgets.Layout(\n",
    "            width=\"100%\",\n",
    "            height=\"300px\",\n",
    "            overflow=\"auto\",\n",
    "            border=\"1px solid #ccc\",\n",
    "            padding=\"5px\",\n",
    "            display=\"flex\",\n",
    "            flex_flow=\"column\"\n",
    "\n",
    "        )\n",
    "    )\n",
    "    # output = widgets.Output(layout={})\n",
    "    text_box = widgets.Text(\n",
    "        placeholder=\"Type your message here\",\n",
    "        description=\"You:\",\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width=\"80%\")\n",
    "    )\n",
    "    submit_button = widgets.Button(\n",
    "        description=\"Send\",\n",
    "        button_style=\"primary\",\n",
    "        layout=widgets.Layout(width=\"10%\")\n",
    "    )\n",
    "\n",
    "    chat_container = widgets.VBox([scrollable_chat, widgets.HBox([text_box, submit_button])])\n",
    "    \n",
    "    # Function to handle submission\n",
    "    def on_submit(_):\n",
    "        with output:\n",
    "\n",
    "            user_message = text_box.value\n",
    "            if user_message.strip():  # Process non-empty input\n",
    "                display(widgets.HTML(\n",
    "                    f\"<div style='color: blue; font-weight: bold;'>You:</div> {text_box.value}\"\n",
    "                ))\n",
    "                bot_response = get_response(user_message)\n",
    "                display(widgets.HTML(\n",
    "                    f\"<div style='color: green; font-weight: bold;'>Bot:</div> {bot_response}\"\n",
    "                ))\n",
    "            text_box.value = \"\"  # Clear the text box after submission\n",
    "        \n",
    "    # Attach event handlers\n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    display(chat_container)\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()\n",
    "\n",
    "# Example question: What is Microsoft's most important contribution to the environment? → this is a question outside the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef79b5",
   "metadata": {},
   "source": [
    "### Interesting case ...\n",
    "\n",
    "<div align=\"left\">\n",
    "    <img \n",
    "        src=\"./img/map.png\" \n",
    "        width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495d817-9786-48e0-bbbf-84b1c899a71b",
   "metadata": {},
   "source": [
    "# Challenge 2: *Getting answers from our new Copilot*\n",
    "\n",
    "## Challenge 2 - Step 1: *Let's take a look at the test dataset*\n",
    "\n",
    "First, let's see an example from the test dataset that we downloaded together with the PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193ef78-b09b-4e60-acc3-851ff7b69859",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = 2\n",
    "\n",
    "def create_instance_md(k, v):\n",
    "    md_str = f\"**{k}:**\\n\\n\"\n",
    "    if k == \"reference_contexts\":\n",
    "        return md_str + \"\\n\".join([f\"* {c}\\n\" for c in v])\n",
    "    return md_str + f\"{v}\"\n",
    "\n",
    "display(Markdown(\"### This is one instance in the dataset:\\n\\n\"))\n",
    "display(Markdown(\"\\n\\n\".join([create_instance_md(k, v) for k,v in rag_dataset.to_pandas().iloc[instance_idx].items()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4765af1-d5b2-4427-8981-62837b41c149",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "In total, the test dataset has 50 instances like the one detailed above, Let's take a look at some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5ace0-0a60-4123-aabd-6f3cfb47a4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rag_dataset.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151b90f-d7d3-43a7-9ac2-7910013e61fb",
   "metadata": {},
   "source": [
    "## Challenge 2 - Step 2: *Let's calculate the responses*\n",
    "\n",
    "Now, let's have the model we created **provide answers to the questions** in the dataset. Also, to be able to calculate the quality metrics, we need to keep the **contexts retrieved**.  The retrieved contexts are the chunks of data retrieved from the semantic index to be used to answer the question.\n",
    "\n",
    "> This process could take hours depending on the thoughtput of the LLM used. Just to keep the practical guide in a reasonable time, let's calculate the **responses** and **retrieved contexts** for a small sample of questions. For future steps, we will provide the full dataset with already pre-calculated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fa1e1-443b-4584-8633-4d848d461f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select a subset sample to test\n",
    "sample_size = 5\n",
    "sub_dataset = LabelledRagDataset(examples=rag_dataset.examples[:sample_size])\n",
    "sub_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18721527-37e9-468f-997f-2e69380571fc",
   "metadata": {},
   "source": [
    "Now, let's calculate the responses for the **subset** (this should take ~1m):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db0141-6337-428e-8ced-b30bdcc19982",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = sub_dataset.make_predictions_with(\n",
    "    predictor = query_engine,\n",
    "    show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9e2f4-3cbd-41ff-8e49-c07bdce14b81",
   "metadata": {},
   "source": [
    "Let's look at the results:\n",
    "\n",
    "> Here, since we are already preparing the data for the next challenge the column names changed:\n",
    "> * `user_input` is the same `query` in the previous format.\n",
    "> * `retrieved_contexts` is a list that includes the chucks of data used by our copilot to answer the question.\n",
    "> * `response` is the `response` to the question created by our new copilot.\n",
    "> * `reference` is the `reference answer` from our previous format and refers to the expected answer created by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70fa284-6975-4679-8553-c7da5d006f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_samples = []\n",
    "\n",
    "for idx in range(len(sub_dataset.examples)):\n",
    "    list_of_samples.append(\n",
    "        SingleTurnSample (\n",
    "            user_input = sub_dataset.examples[idx].query,\n",
    "            reference = sub_dataset.examples[idx].reference_answer,\n",
    "            response = predictions.predictions[idx].response,\n",
    "            retrieved_contexts = predictions.predictions[idx].contexts\n",
    "        )\n",
    "    )\n",
    "\n",
    "ragas_evaluation_dataset = EvaluationDataset(list_of_samples)\n",
    "ragas_evaluation_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6d120-e5e7-47d4-88f5-114ff658a067",
   "metadata": {},
   "source": [
    "# Challenge 3: *Evaluate the quality of our new copilot*\n",
    "\n",
    "Evaluating Generative AI (GenAI) solutions, especially those using Retrieval-Augmented Generation (RAG) frameworks, involves several key steps to ensure the quality and effectiveness of the generated outputs. \n",
    "\n",
    "GenAI solutions are evaluated using various metrics to measure the quality of the responses generated by the AI models. Libraries like RAGAS or promptflow are instrumental in this process. They help assess the solution's performance using metrics such as faithfulness, groundedness, context precision, and context recall1. These metrics provide insights into how accurate, relevant, and contextually appropriate the AI-generated responses are.\n",
    "\n",
    "The evaluation process typically includes:\n",
    "\n",
    "* **Model Selection and Building:** Choosing the right model and developing the GenAI application using iterative development practices and advanced prompt engineering techniques.\n",
    "* **Quality Metrics:** Using metrics like faithfulness (accuracy and truthfulness), groundedness (relevance to the input context), context precision (alignment with the provided context), and context recall (inclusion of relevant information from the input context).\n",
    "* **Explainability:** Adding explainability to these metrics to understand why certain responses are rated higher or lower. This helps in debugging and improving the overall solution.\n",
    "* **User Feedback:** Collecting feedback from users to identify unexpected issues and improve the user experience.\n",
    "\n",
    "For this demonstration, we use RAGAS, a specialized evaluation framework designed to assess the performance of RAG systems. It provides a structured approach to evaluate the effectiveness of RAG implementations by leveraging advanced Large Language Models (LLMs) as judges. \n",
    "\n",
    "## Challenge 3 - Step 1: *Initialize the LLM and Embedding models*\n",
    "\n",
    "The first step is to initialize the LLM and Embedding models to be used to calculate the GenAI metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d20855-309e-4e36-a07c-9ea551d41a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_llm = LlamaIndexLLMWrapper(llm)\n",
    "evaluator_embeddings = LlamaIndexEmbeddingsWrapper(embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027f6ef-80c0-495e-8096-dfb48c6d3b47",
   "metadata": {},
   "source": [
    "## Challenge 3 - Step 2: *Calculate the GenAI metrics*\n",
    "\n",
    "The process to calculate the GenAI metrics for all the questions in the test dataset could take hours depending on the throughtput of the LLM and the Embedding model used. To keep the time of this guide we are going to calculate only 3 metrics for the sub dataset created before:\n",
    "\n",
    "* **Context precision:** Measures how much of the generated output is relevant and aligns with the context provided in the input.\n",
    "* **Context recall:** Measures how much of the relevant information in the input context is included in the output.\n",
    "* **Faithfulness:** Measures how accurate and truthful the generated output is in relation to the input context and factual correctness. Faithfulness is about avoiding \"hallucinations\" (made-up or false information).\n",
    "\n",
    "\n",
    "> This process can take up to **25 seconds** to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe2de4-576b-4d3b-bf26-1885a4f91411",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "metrics = [\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    ContextPrecision(llm=evaluator_llm),\n",
    "    ContextRecall(llm=evaluator_llm)\n",
    "]\n",
    "ragas_evaluation_result = evaluate(\n",
    "    dataset=ragas_evaluation_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings,\n",
    "    run_config=RunConfig(timeout=1800, max_wait=180, max_retries=20),\n",
    "    show_progress=True,\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec49e5-c34a-405d-8120-48bf9293329f",
   "metadata": {},
   "source": [
    "The following are the results of the calculation of the GenAI metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35bb08-587e-4c7e-a8a6-5d4526d9fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ragas_result = ragas_evaluation_result.to_pandas()\n",
    "df_ragas_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cf0a4-06e5-4060-b983-dc993c5c3862",
   "metadata": {},
   "source": [
    "### Full dataset results\n",
    "\n",
    "Let's take a look at a table with the responses, retrieved contexts and metrics already pre-calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8824016-51b4-457a-ad37-3b840c6b3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset = pd.read_json('./test-dataset.json', orient='records')\n",
    "df_test_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e5b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
