{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e195d69-1f13-432c-b2a9-3e2ada6e7fe7",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "    <img \n",
    "        src=\"./img/Microsoft-Logo.png\" \n",
    "        width=\"400\"/>\n",
    "</h1>\n",
    "<h1 align=\"center\">\n",
    "    <b>Practical Guide</b>\n",
    "</h1>\n",
    "<h4 align=\"center\">\n",
    "    for the creation of an AI Solution using an accelerator from the <a href=\"https://www.ds-toolkit.com/\">Data Science Toolkit</a>\n",
    "</h4>\n",
    "\n",
    "# What to expect\n",
    "\n",
    "* **Challenge 1:** *Automatically determine relevant features from the set of questions.*\n",
    "* **Challenge 2:** *From the dataset, fill out the dataset of the new features defined.*\n",
    "* **Challenge 3:** *Fit and evaluate regression models using the features of the previous step as inputs and the metrics evaluated as outputs. Then apply SHAP to the newly created models.*\n",
    "\n",
    "# Challenge 1: *Determine relevant features from the questions*\n",
    "\n",
    "Here we will use the set of questions and leverage an LLM to determine what would be good features to try and understand the resulting metrics.\n",
    "\n",
    "## Challenge 1 - Step 1:  Let's import the required packages and libraries.\n",
    "\n",
    "> This is going to be done in a quiet mode, and only errors will be displayed if they occur. If you like to see what is going to be installed look at the [requirements.txt](./requirements.txt) file.\n",
    "\n",
    "In summary two main tools will be installed that will be used in this notebook:\n",
    "\n",
    "* **genAISHAP**. Is the library containing the tools for the DS Toolkit.\n",
    "* **shap**. A popular library used to help with interpretability.\n",
    "\n",
    "> Estimated time: 8.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0be14-b7c8-4203-9ec3-6ed801cb4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from genaishap import Featurizer, GenAIExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "LOAD_PRECALCULATED_VALUES = True\n",
    "\n",
    "load_dotenv()\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9d491-dd95-4e29-943f-8492b355c4dc",
   "metadata": {},
   "source": [
    "### Some definitions:\n",
    "\n",
    "* **Context prescision:** Measures how much of the generated output is relevant and aligns with the context provided in the input.\n",
    "* **Context recall:** Measures how much of the relevant information in the input context is included in the output.\n",
    "* **Faithfulness:** Measures how accurate and truthful the generated output is in relation to the input context and factual correctness. Faithfulness is about avoiding \"hallucinations\" (made-up or false information).\n",
    "\n",
    "Below, the dataset of questions, retrieved contexts, generated and expected responses and their corresponding metrics is presented. Remember that this dataset is generated following the same procedure explained in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5146b6-ab7b-41f6-b5fc-ff1f5118edd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_dataset = pd.read_json('./test-dataset.json', orient='records')\n",
    "df_test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4cdcb",
   "metadata": {},
   "source": [
    "See an example question below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset.iloc[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18902d53-47b4-4d69-9bf1-2683004bbd29",
   "metadata": {},
   "source": [
    "## Challenge 1 - Step 2: Let's extract features from the questions in the dataset\n",
    "\n",
    "* To do this, we will use the function `Featurizer` from the **genAISHAP** library.\n",
    "* The features created are then displayed. Remember that these features are generated automatically.\n",
    "\n",
    "**GenAISHAP** has a utility to automatically create features from the `user_input` entries. This tool, called **featurizer**, works by using an LLM to go through the existing questions in the dataset and extracts what would be relevant pieces of information that would be useful as features in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19761051-1c51-4a80-82c5-5d64f86a63c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# The Featurizer is part of the DS Toolkit and is able to take the list of provided questions and create a dataset of features for them automatically.\n",
    "featurizer = Featurizer.from_pandas(df_test_dataset)\n",
    "featurizer.create_features_using_azure_openai(\n",
    "    deployment_name=\"gpt-4o\", # Update with the name of your Azure OpenAI LLM deployment name\n",
    "    num_features=25\n",
    ")\n",
    "print(featurizer.features.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac672b70-de87-4243-bfb8-52273b82b3ee",
   "metadata": {},
   "source": [
    "# Challenge 2: From the dataset, fill out the dataset of the new features defined.\n",
    "\n",
    "**GenAISHAP** also includes another utility to automatically fill out the values for each user input for each feature. Once more, we leverage an LLM to fill out each of the features for each of the questions. It works like answering questions about the question (e.g., if the feature is `is_Microsoft_mentioned`, it literally checks if Microsoft is mentioned in the question).\n",
    "\n",
    "> Estimated time: 55s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161095d-053d-4353-9edb-1adb9a6f502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.fill_out_features_using_azure_openai(\n",
    "    deployment_name=\"gpt-4o\", \n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a7769-0ee6-468e-8c6c-684a0cabb380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We provide the option to use a precalculated set of features to speed up the process\n",
    " \n",
    "if LOAD_PRECALCULATED_VALUES:\n",
    "    df_features = pd.read_json('./test-features.json', orient='records')\n",
    "else:\n",
    "    df_features = featurizer.to_pandas()\n",
    "    \n",
    "df_test_dataset.join(df_features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a903ad2-185d-4582-889c-098312035ef8",
   "metadata": {},
   "source": [
    "# Challenge 3: \n",
    "## Challenge 3 - Step 1: Fit and evaluate regression models using the features of the previous step as inputs and the metrics evaluated as outputs.\n",
    "\n",
    "**GenAISHAP** will create regression models, which we call black-box models, for each of the metrics using the features as input variables and the calculated metrics as output variables:\n",
    "\n",
    "$ \\hat{\\mathbf{y}} = f(\\mathbf{X})$\n",
    "\n",
    "With $\\hat{\\mathbf{y}}$ the metric calculated by the black-box model $f()$ and the regressors $\\mathbf{X}$ as inputs. Note that $\\mathbf{X}$ are the features automatically generated in the previous steps, while the original $\\mathbf{y}$ are the metrics obtained from RAGAS in the previous notebook.\n",
    "\n",
    "Then, it will use those black-box models to produce explanations for each metric using SHAP.\n",
    "\n",
    "The following cell includes 3 lines:\n",
    "* The first line initialices the object `GenAIExplainer` with the information from the test dataset and the features generated in the previous steps.\n",
    "* The next one executes the **feaure engineering** where the features are converted to numerical features mainly using **one-hot vector encoding**.\n",
    "* Finally, multiple regression models are trained and optimized for each metric and the best is chosen for each metric in order to create the **SHAP explainers**.\n",
    "\n",
    "\n",
    "> Estimated time: 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7aa24e-300c-4ab0-8341-9851d0787368",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_explainer = GenAIExplainer.from_pandas(df_test_dataset, df_features)\n",
    "genai_explainer.feature_engineering()\n",
    "genai_explainer.create_explainers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6c43e-1d4c-4d38-927b-6994344f8354",
   "metadata": {},
   "source": [
    "### Show the `r2 score` of the selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19574935-a689-48ba-8fa9-2452376d4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_explainer.r2_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8f6f3",
   "metadata": {},
   "source": [
    "### Let's select one of our metrics\n",
    "1. Select on of the metrics from the dropdown menu below.\n",
    "2. Check how well the regression model created with the automated features follows the selected metric. This should give us an idea about how reliable our explanations are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_values = [\"\", \"faithfulness\", \"context_precision\", \"context_recall\"]\n",
    "\n",
    "# Create a dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=dropdown_values,\n",
    "    description='Select:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to handle the dropdown selection\n",
    "def on_change(change):\n",
    "    global sel_metric\n",
    "    global metric\n",
    "    global X\n",
    "    global metric_text\n",
    "    global df_metric\n",
    "    sel_metric = change['new']\n",
    "    # print(f'Selected metric: {sel_metric}')\n",
    "\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Display the dropdown widget again\n",
    "    display(dropdown)\n",
    "\n",
    "    if sel_metric == \"faithfulness\":\n",
    "        metric_text = \"Measures how accurate and truthful the generated output is in relation to the input context and factual correctness. Faithfulness is about avoiding hallucinations (made-up or false information)\"\n",
    "    elif sel_metric == \"context_precision\":\n",
    "        metric_text = \"Measures how much of the generated output is relevant and aligns with the context provided in the input.\"\n",
    "    elif sel_metric == \"context_recall\":\n",
    "        metric_text = \"Measures how much of the relevant information in the input context is included in the output.\"\n",
    "    \n",
    "    metric_details = f\"\"\"\n",
    "### **{sel_metric}:** {metric_text}\n",
    "\"\"\"\n",
    "    # Display a reminder of the metric's definition\n",
    "    display(Markdown(metric_details))\n",
    "\n",
    "    # Plot the actual vs estimated values for the selected metric\n",
    "    metric = sel_metric\n",
    "    X = pd.DataFrame(genai_explainer.preprocessed_features)\n",
    "\n",
    "    df_metric = pd.DataFrame(genai_explainer.metrics)[[metric]]\n",
    "    df_metric['estimated_value'] = genai_explainer.estimators_[metric].predict(X)\n",
    "    df_metric['is_out_of_range'] = genai_explainer.is_out_of_range_[metric]\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(df_metric[metric], label='Actual Value', marker='o')\n",
    "    plt.plot(df_metric['estimated_value'], label='Estimated Value', marker='s')\n",
    "\n",
    "    # Highlight the out-of-range values\n",
    "    out_of_range_indices = df_metric[df_metric['is_out_of_range']].index\n",
    "    plt.scatter(out_of_range_indices, df_metric.loc[out_of_range_indices, 'estimated_value'], color='red', label='Out of Range', zorder=5)\n",
    "\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'{metric} vs Estimated Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "# Attach the function to the dropdown widget\n",
    "dropdown.observe(on_change, names='value')\n",
    "\n",
    "\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144273b2-d664-4e30-9def-afe7fae654df",
   "metadata": {},
   "source": [
    "## Challenge 3 - Step 2: Present the explainability results from SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a method used to explain the output of machine learning models. It leverages concepts from cooperative game theory to assign each feature an importance value for a particular prediction. SHAP values provide insights into how each feature contributes to the model's output, making it easier to understand and interpret complex models.\n",
    "In this case, we will use SHAP on top of the models of the metrics we created in the previous steps.\n",
    "\n",
    "### For the selected metric show the SHAP values of each feature.\n",
    "\n",
    "`shap.summary_plot` is a function in the SHAP library that visualizes the importance of features in a machine learning model. It provides a summary of the SHAP values for all features, using a combination of dot plots and bar charts. This plot helps to quickly identify which features have the most significant impact on the model's predictions.\n",
    "\n",
    "In the plot below each dot represents a SHAP value for a feature in a specific instance (data point). Here's how to interpret the dots and colors:\n",
    "\n",
    "* Dots:\n",
    "Each dot corresponds to a single instance's SHAP value for a particular feature.\n",
    "The **position** of the dot on the x-axis shows the **SHAP value**, indicating the impact of that feature on the prediction. Dots further to the right (positive SHAP values) indicate a positive impact on the prediction, while dots further to the left (negative SHAP values) indicate a negative impact.\n",
    "* Colors:\n",
    "The **color** of each dot represents the **feature value** for that instance.\n",
    "In our case, a color gradient (e.g., blue to red) is used, where one end of the spectrum (i.e., blue) represents low feature values and the other end (i.e., red) represents high feature values.\n",
    "This coloring helps to understand the relationship between the feature value and its impact on the prediction. For example, if red dots (high feature values) are mostly on the right, it indicates that high values of that feature increase the prediction.\n",
    "\n",
    "By examining the distribution and color of the dots, you can gain insights into how different feature values influence the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5af37-1326-4ad0-81ad-997258c472a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_explainer = genai_explainer.explainers_[metric]\n",
    "shap_values = metric_explainer(X)\n",
    "shap.summary_plot(shap_values, X, plot_size=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc9186-9af2-48f0-b259-1d7f78ce6ad5",
   "metadata": {},
   "source": [
    "### Now let's select one question\n",
    "\n",
    "For the selected question, display the question, the retrieved contexts, the generated and expected answers, the metric value and the predicted metric from the model trained\n",
    "\n",
    "Also, show the SHAP values for the selected question (for the previously selected metric). \n",
    "`shap.waterfall_plot` is a function in the SHAP library that visualizes the contribution of **each feature to a single prediction**. It breaks down the prediction into the base value (average model output) and the impact of each feature's SHAP value, showing how each feature pushes the prediction higher or lower. This plot helps to understand the specific reasons behind an individual prediction by displaying the cumulative effect of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae969a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dropdown values\n",
    "dropdown_values = df_test_dataset.iloc[:, 0].tolist()\n",
    "\n",
    "# Create a dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[(value, index) for index, value in enumerate(dropdown_values)],\n",
    "    description='Select:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to handle the dropdown selection\n",
    "def on_change(change):\n",
    "    global sel_question\n",
    "    sel_question = change['new']\n",
    "    # print(f'Selected question: {sel_question}')\n",
    "\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Display the dropdown widget again\n",
    "    display(dropdown)\n",
    "\n",
    "    # Display the details of the selected question\n",
    "    index = sel_question\n",
    "\n",
    "    context = df_test_dataset.loc[index,'retrieved_contexts']\n",
    "    context_str = \"\\n\".join([f\"\\n**CHUNK {i+1}:**\\n\\n{c}\" for i, c in zip(range(len(context)),context)])\n",
    "\n",
    "    index_details = f\"\"\"\n",
    "### INDEX {index}\n",
    "\n",
    "**USER INPUT:**\n",
    "{df_test_dataset.loc[index,'user_input']}\n",
    "\n",
    "**RETRIEVED CONTEXT:**\n",
    "\n",
    "{context_str}\n",
    "\n",
    "**RESPONSE:**\n",
    "{df_test_dataset.loc[index,'response']}\n",
    "\n",
    "**REFERENCE:**\n",
    "{df_test_dataset.loc[index,'reference']}\n",
    "\n",
    "**METRIC â†’ {metric} :** {metric_text}\n",
    "\n",
    "**METRIC Value:** {df_test_dataset.loc[index, metric]:.3f}\n",
    "\n",
    "**MODEL ESTIMATED Value:** {df_metric.loc[index, 'estimated_value']:.3f}\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(index_details))\n",
    "    shap.waterfall_plot(shap_values[index])\n",
    "\n",
    "# Attach the function to the dropdown widget\n",
    "dropdown.observe(on_change, names='value')\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b85f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
